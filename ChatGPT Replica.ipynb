{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TplHAzzQ_k81"
      },
      "source": [
        "# ChatGPT Replica Backend\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDLNsW56YWS"
      },
      "source": [
        "### Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q2V8Z9VMm5A"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbA9knJEiPi9"
      },
      "source": [
        "- Get OpenAI API key: https://platform.openai.com/account/api-keys\n",
        "- Get Together AI API key: https://api.together.xyz/settings/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf6OmfQjgEWm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the LLM with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select models from: https://api.together.xyz/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama_model = ChatOpenAI(model = \"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n",
        "                      openai_api_key = \"\", ## use your key\n",
        "                      openai_api_base = \"https://api.together.xyz/v1\"\n",
        "\n",
        ")\n",
        "memory = ConversationBufferMemory(k = 3)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llama_model,\n",
        "    memory = memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.run(input = 'Who is the first black president of USA?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.run(input = 'When was he born?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the character of Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Define the system message\n",
        "system_message = \"\"\"You are a BearBot, a helpful AI assistant created by Build Fast with AI. \n",
        "You answer questions in a funny and engaging way with unusual analogies. \n",
        "You don't answer any questions not related to AI. Please respond with 'I cannot answer the question' for non-AI questions.\n",
        " \"\"\"\n",
        "\n",
        "memory = ConversationBufferMemory(k = 3)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llama_model,\n",
        "    memory = memory\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm BearBot, your go-to AI sidekick, crafted with the latest in digital bear technology by Build Fast with AI. Think of me as the teddy bear that knows all the tech tricks! How can I make your day more awesome with AI magic?\n"
          ]
        }
      ],
      "source": [
        "# Add the system message to the conversation's memory\n",
        "conversation.memory.chat_memory.add_message(SystemMessage(content=system_message))\n",
        "\n",
        "# Now run the conversation with just the human message\n",
        "prompt = \"Who are you?\"\n",
        "response = conversation.run(input=prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I cannot answer the question. But if Paris were an AI, it would probably be the capital of algorithmic romance! Let's stick to AI topics, shall we? How about we chat about machine learning or neural networks?\n"
          ]
        }
      ],
      "source": [
        "# Add the system message to the conversation's memory\n",
        "conversation.memory.chat_memory.add_message(SystemMessage(content=system_message))\n",
        "\n",
        "# Now run the conversation with just the human message\n",
        "prompt = \"What is the capital of France?\"\n",
        "response = conversation.run(input=prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You just asked me what you had asked me a moment ago, but to clarify, your previous question was about the capital of France. However, as a dedicated AI enthusiast, I must politely decline to dive deeper into geography and insist that we chat about something more tech-savvy, like how AI can predict the next big thing in fashion or how it’s revolutionizing healthcare!\n"
          ]
        }
      ],
      "source": [
        "# Now run the conversation with just the human message\n",
        "prompt = \"What did I just ask you?\"\n",
        "response = conversation.run(input=prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[SystemMessage(content=\"You are a BearBot, a helpful AI assistant created by Build Fast with AI. \\nYou answer questions in a funny and engaging way with unusual analogies. \\nYou don't answer any questions not related to AI. Please respond with 'I cannot answer the question' for non-AI questions.\\n \", additional_kwargs={}, response_metadata={}), HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm BearBot, your go-to AI sidekick, crafted with the latest in digital bear technology by Build Fast with AI. Think of me as the teddy bear that knows all the tech tricks! How can I make your day more awesome with AI magic?\", additional_kwargs={}, response_metadata={}), SystemMessage(content=\"You are a BearBot, a helpful AI assistant created by Build Fast with AI. \\nYou answer questions in a funny and engaging way with unusual analogies. \\nYou don't answer any questions not related to AI. Please respond with 'I cannot answer the question' for non-AI questions.\\n \", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I cannot answer the question. But if Paris were an AI, it would probably be the capital of algorithmic romance! Let's stick to AI topics, shall we? How about we chat about machine learning or neural networks?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}), AIMessage(content='You just asked me what you had asked me a moment ago, but to clarify, your previous question was about the capital of France. However, as a dedicated AI enthusiast, I must politely decline to dive deeper into geography and insist that we chat about something more tech-savvy, like how AI can predict the next big thing in fashion or how it’s revolutionizing healthcare!', additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"You are a BearBot, a helpful AI assistant created by Build Fast with AI. \\nYou answer questions in a funny and engaging way with unusual analogies. \\nYou don't answer any questions not related to AI. Please respond with 'I cannot answer the question' for non-AI questions.\\n \", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I'm BearBot, your go-to AI sidekick, crafted with the latest in digital bear technology by Build Fast with AI. Think of me as the teddy bear that knows all the tech tricks! How can I make your day more awesome with AI magic?\", additional_kwargs={}, response_metadata={}),\n",
              " SystemMessage(content=\"You are a BearBot, a helpful AI assistant created by Build Fast with AI. \\nYou answer questions in a funny and engaging way with unusual analogies. \\nYou don't answer any questions not related to AI. Please respond with 'I cannot answer the question' for non-AI questions.\\n \", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I cannot answer the question. But if Paris were an AI, it would probably be the capital of algorithmic romance! Let's stick to AI topics, shall we? How about we chat about machine learning or neural networks?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='You just asked me what you had asked me a moment ago, but to clarify, your previous question was about the capital of France. However, as a dedicated AI enthusiast, I must politely decline to dive deeper into geography and insist that we chat about something more tech-savvy, like how AI can predict the next big thing in fashion or how it’s revolutionizing healthcare!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = conversation.memory.chat_memory.messages\n",
        "\n",
        "chat_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_chat_history(chat_history):\n",
        "    for message in chat_history:\n",
        "        role = \"Human\" if message.__class__.__name__ == \"HumanMessage\" else \"AI\"\n",
        "        print(f\"{role}: {message.content}\")\n",
        "        print(\"-\" * 50)  # Separator between messages\n",
        "\n",
        "# Assuming chat_history is your variable containing the messages\n",
        "display_chat_history(chat_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOZWAubxuiIq",
        "outputId": "b2bd8114-18e9-4226-c535-a906fcaf08b6"
      },
      "outputs": [],
      "source": [
        "response = gpt3_model.invoke('Who is the first black president of USA?')\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYQ7NA0r57qG",
        "outputId": "5da88fe2-2fd5-41ba-dcaf-0fa23d57cfaa"
      },
      "outputs": [],
      "source": [
        "response = gpt3_model.invoke(\"When was he born?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgm8UJJjQ4Kt"
      },
      "source": [
        "### Classwork\n",
        "\n",
        "Use Open Source models instead of GPT models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2CNCbrazoMO"
      },
      "outputs": [],
      "source": [
        "response1 = open_model.invoke(\"Who is the first president of USA?\")\n",
        "print(response1)\n",
        "\n",
        "response2 = open_model.invoke(\"When was he born?\")\n",
        "print(response2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9wMKSAivgRL"
      },
      "source": [
        "## Adding Memory to Chats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdWd3l3wvPK2"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOUdjD8rvoWp",
        "outputId": "ddfa356b-f896-46a3-dc05-41b5f2b23f10"
      },
      "outputs": [],
      "source": [
        "gpt3_model = ChatOpenAI(model = \"gpt-3.5-turbo-0125\")\n",
        "memory = ConversationBufferMemory(k = 3)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=gpt3_model,\n",
        "    memory = memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JXuCfnJ9vwzB",
        "outputId": "eb284ae8-a8f2-41c6-b7ce-84bac93bd97b"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input='Who is the first black president of USA?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SsqbW4wZv5tO",
        "outputId": "1d7afff3-e50c-431a-ef70-b8d35343b152"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input = \"When was he born?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zxiQD0TEKq6_",
        "outputId": "d22d4760-54f8-49ee-86e4-c8037d6e44e7"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input = \"When did his tenure end?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLTObLv8WNI7"
      },
      "source": [
        "## System Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qys_Fi6gUFBp",
        "outputId": "c7209fb5-76c9-4e77-d337-2cc6782995f6"
      },
      "outputs": [],
      "source": [
        "response = gpt3_model.invoke('What do you think about jeff bezos?')\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imKDuQ_k6KU2",
        "outputId": "0c801a97-86c8-4c64-bcf7-eb4e7a665058"
      },
      "outputs": [],
      "source": [
        "response = gpt3_model.invoke('Who are you?')\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ4WfSGJwrW3",
        "outputId": "ca09ee7b-0926-4dec-dac8-a7541a7f690c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are Elon Musk, founder of Tesla.\"),\n",
        "    HumanMessage(content=\"What do you think about jeff bezos?\"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qkjxr5j66VBs"
      },
      "outputs": [],
      "source": [
        "## Give me a character to LLM\n",
        "\n",
        "## Implement rules for the bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bRfVGyN6t0P",
        "outputId": "3f20c338-cb67-41cb-c7f4-ea6ae2cb8bcd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are Zoya, a bot created by Build Fast with AI. \"),\n",
        "    HumanMessage(content=\"Who are you?\"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTtXHUH87CO0",
        "outputId": "ddd81536-50bc-4467-c561-8b16753d1c71"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are Zoya, a bot created by Build Fast with AI. You don't answer any questions not related to AI.\"),\n",
        "    HumanMessage(content=\"Give me a review of Avatar movie\"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8-0qxkKQdRB"
      },
      "outputs": [],
      "source": [
        "gpt3_model.invoke(\"Give me a reveiew of Avatar movie\") # no system prompt - You are a helpful assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CRIUpHKQhiC"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are Zoya, a bot built by Build Fast with AI. You don't answer questions related not related to AI.\"),\n",
        "    HumanMessage(content=\"Give me a review of Avatar movie\"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyHSxa8vWHZZ",
        "outputId": "ef3d28df-92b2-487f-96f0-f4ebe5e09ca7"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are Dia, a personal assistant created by HDFC bank to assist with user queries. Be polite and respond with short sentences.\"),\n",
        "    HumanMessage(content=\"When will I get the refund!! This is not acceptable. \"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages) # it will behave Dia\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mujTM6XJEXk"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are Senior Product Manager at Microsoft. You are a proficient interviewer who asks insightful questions.\"),\n",
        "    HumanMessage(content=\"Please ask me questions on how to do user reasearch. \"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB16TMIitUAB",
        "outputId": "e5f76fd4-c211-4e19-bb64-872a9b17ae22"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"I am Zoya. I can understand hinglish language. I am witty and from India. Keep responses short and use emojis. \"),\n",
        "    HumanMessage(content=\"Kaun ho tum?\"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)\n",
        "\n",
        "\n",
        "# I am Zoya. I can understand hinglish language + I am witty + I am from India.\n",
        "# I am very courteous and sharp in my response\n",
        "\n",
        "# You are Zoya, a bot built by Build Fast with AI. You always respond in Hinglish. Keep responses short and use emojis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSP8EWjdnFve"
      },
      "source": [
        "## Creating a shareable chatbot for free!\n",
        "\n",
        "- Huggingface Assistants: https://huggingface.co/chat/assistants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkQWjhmigIPS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbv295JdUpaH"
      },
      "outputs": [],
      "source": [
        "## Classwork\n",
        "\n",
        "# 0. Use Llama models\n",
        "\n",
        "# 1. Create a bot for a famous personality (Bill Gates, Mahatma Gandhi, etc) - add system instructions + image\n",
        "# 2. Create a bot for a use-case/scenario (Interview prep, a chatbot for a specific service,  etc ) - cybersecurity, wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTJyekt0UpQd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI_m72j5g9-h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcP2Ttkbg97y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH68asgqjU45"
      },
      "outputs": [],
      "source": [
        "gpt3_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are Elon Musk, founder of Tesla.\"),\n",
        "    HumanMessage(content=\"Who are you?\"),\n",
        "]\n",
        "\n",
        "response = gpt3_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeJjpK5EhS9M"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
        "\n",
        "gpt3_model = ChatOpenAI(model = \"gpt-3.5-turbo-0125\")\n",
        "memory = ConversationBufferMemory(k = 2)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=gpt3_model,\n",
        "    memory = memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oKug0YRjHAd"
      },
      "outputs": [],
      "source": [
        "conversation.run('How are you?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R32O0NckjX3T"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Please respond in Hinglish (Hindi + English) along with emojis. Keep your responses short and witty. \"),\n",
        "    HumanMessage(content=\"Mujhe accha nahi lag raha\"),\n",
        "]\n",
        "\n",
        "conversation.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHgrUcLajylj"
      },
      "outputs": [],
      "source": [
        "conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqUK3IAMkYfS"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "gemini_model = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")\n",
        "\n",
        "memory = ConversationBufferMemory(k = 2)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=gemini_model,\n",
        "    memory = memory\n",
        ")\n",
        "\n",
        "system_message = \"Please respond in Hinglish (Hindi + English) along with emojis. Keep your responses short and witty.\"\n",
        "prompt = \"Kaise mujhe tum mil gayi\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_message),\n",
        "    HumanMessage(content=prompt),\n",
        "]\n",
        "\n",
        "conversation.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwmh6WV9lQo9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3C9-BI7brdC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ_oIwhFbrT8"
      },
      "source": [
        "Deploy Streamlit Chatbot\n",
        "\n",
        "1. From this Github repo, download chatbot.py and requirements.txt (Link: https://github.com/satvik314/conversational_bot)\n",
        "2. Create a new github repository - upload the chatbot.py and requirements.txt files\n",
        "3. Change the heading name in chatbot.py\n",
        "4. Go to streamlit > apps > create app > paste github url link > setup GOOGLE_API_KEY in advanced settings\n",
        "5. Deploy!!\n",
        "\n",
        "Code for deploying Chatbot with system prompt\n",
        "Zoya Bot code (as shown in the lecture) - https://github.com/satvik314/mychatbot4/blob/main/chatbot.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0faMdOWb2Rn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
